"""
Processes task folders generated by the FSM-LLM-Narrative system,
extracts the clean JSON content from the 'raw_response' field of log files,
and saves it to a parallel structure under a 'Middle_data' directory.
"""

import os
import json
import glob
import argparse
import re
from typing import Dict, List, Optional, Any

# Default base directory for the processed data
DEFAULT_MIDDLE_DATA_DIR = "Middle_data"

def clean_and_parse_raw_response(raw_response_str: str) -> Optional[Any]:
    """Cleans markdown fences and parses JSON string."""
    if not isinstance(raw_response_str, str):
        print(f"Warning: raw_response is not a string ({type(raw_response_str)}), skipping parsing.")
        return None
        
    cleaned_response = raw_response_str.strip()
    if cleaned_response.startswith("```json"):
        cleaned_response = cleaned_response[7:]
    if cleaned_response.endswith("```"):
        cleaned_response = cleaned_response[:-3]
    cleaned_response = cleaned_response.strip()

    if not cleaned_response:
        # print("Warning: Cleaned response is empty, skipping parsing.")
        return None # Return None for empty strings after cleaning

    try:
        return json.loads(cleaned_response)
    except json.JSONDecodeError as e:
        print(f"Warning: JSONDecodeError - {e}. Content: {cleaned_response[:150]}...")
        return None
    except Exception as e:
        print(f"Warning: Unexpected error parsing JSON - {e}. Content: {cleaned_response[:150]}...")
        return None

def extract_task_details_from_file(tasks_json_path: str) -> Optional[List[Dict[str, Any]]]:
    """
    Extracts specific task details from a tasks.json file.
    The format is based on the provided image example.
    Assumes tasks.json contains a list of tasks, or a single task object.
    """
    try:
        with open(tasks_json_path, 'r', encoding='utf-8') as f:
            raw_content = f.read()
        data = json.loads(raw_content)
    except FileNotFoundError:
        # File not existing is a common case, not necessarily an error for this function.
        return None
    except json.JSONDecodeError as e:
        print(f"Warning: JSONDecodeError in {tasks_json_path} - {e}. Content snippet: {raw_content[:200]}...")
        return None
    except Exception as e:
        print(f"Warning: Unexpected error processing file {tasks_json_path} - {e}.")
        return None

    extracted_tasks = []
    # Ensure data is processed as a list, even if tasks.json contains a single object
    data_to_process = [data] if not isinstance(data, list) else data

    for task_data in data_to_process:
        if not isinstance(task_data, dict):
            print(f"Warning: Skipping non-dictionary item in {tasks_json_path}")
            continue

        extracted_task = {
            "name": task_data.get("name"),
            "location": task_data.get("location"),
            "environment": task_data.get("environment"),
            "interactive_environment_objects": task_data.get("interactive_environment_objects"),
            "trigger_conditions": task_data.get("trigger_conditions"), # Copies the whole dict if present
            "scene_end_state_reference": task_data.get("scene_end_state_reference") # Copies the whole dict if present
        }

        # Process 'interactive_npc'
        npcs_data = task_data.get("interactive_npc", [])
        extracted_npcs = []
        if isinstance(npcs_data, list):
            for npc in npcs_data:
                if isinstance(npc, dict):
                    extracted_npc_info = {
                        "name": npc.get("name"),
                        "additional_conditions": npc.get("additional_conditions"),
                        "goal": npc.get("goal")
                    }
                    # Process 'emotion_pool' for each NPC
                    emotion_pool_data = npc.get("emotion_pool", [])
                    extracted_emotion_pool = []
                    if isinstance(emotion_pool_data, list):
                        for emotion in emotion_pool_data:
                            if isinstance(emotion, dict):
                                extracted_emotion_pool.append({
                                    "id": emotion.get("id"),
                                    "trigger_condition": emotion.get("trigger_condition"),
                                    "goal": emotion.get("goal")
                                })
                    extracted_npc_info["emotion_pool"] = extracted_emotion_pool
                    extracted_npcs.append(extracted_npc_info)
        extracted_task["interactive_npc"] = extracted_npcs

        # Process 'key_questions'
        key_questions_data = task_data.get("key_questions", [])
        extracted_key_questions = []
        if isinstance(key_questions_data, list):
            for kq in key_questions_data:
                if isinstance(kq, dict):
                    extracted_key_questions.append({
                        "id": kq.get("id"),
                        "content": kq.get("content")
                    })
        extracted_task["key_questions"] = extracted_key_questions
        
        extracted_tasks.append(extracted_task)

    return extracted_tasks if extracted_tasks else None

def process_task_folder_to_middle(task_folder: str, middle_data_base: str, tasks_json_processing_mode: str = "extracted"):
    """
    Processes a single task folder, extracting raw responses and saving clean JSON.
    Also handles tasks.json based on the specified mode.

    Args:
        task_folder: Path to the source task folder (e.g., data/TaskName_Timestamp).
        middle_data_base: Path to the base directory for output (e.g., Middle_data).
        tasks_json_processing_mode: Mode for handling tasks.json ('extracted' or 'raw').
    """
    if not os.path.isdir(task_folder):
        print(f"Error: Source task folder not found: {task_folder}")
        return

    task_folder_basename = os.path.basename(task_folder)
    target_task_dir = os.path.join(middle_data_base, task_folder_basename)

    print(f"Processing task folder: {task_folder_basename}")
    print(f"Target directory: {target_task_dir}")

    # Handle tasks.json from the root of the task folder based on the mode
    tasks_json_original_path = os.path.join(task_folder, "tasks.json")
    if os.path.exists(tasks_json_original_path):
        os.makedirs(target_task_dir, exist_ok=True) # Ensure dir exists

        if tasks_json_processing_mode == "extracted":
            extracted_data = extract_task_details_from_file(tasks_json_original_path)
            if extracted_data:
                target_file_path = os.path.join(target_task_dir, "tasks_details_extracted.json")
                try:
                    with open(target_file_path, 'w', encoding='utf-8') as f_out:
                        json.dump(extracted_data, f_out, indent=2, ensure_ascii=False)
                    print(f"  Successfully wrote extracted task details to: {target_file_path}")
                except Exception as e:
                    print(f"  Error writing extracted task details {target_file_path}: {e}")
            # else: extract_task_details_from_file would have printed a warning if applicable
        elif tasks_json_processing_mode == "raw":
            try:
                with open(tasks_json_original_path, 'r', encoding='utf-8') as f_in:
                    raw_task_content = json.load(f_in) # Parse to validate and allow pretty dump
                
                target_file_path = os.path.join(target_task_dir, "tasks_content_raw.json")
                with open(target_file_path, 'w', encoding='utf-8') as f_out:
                    json.dump(raw_task_content, f_out, indent=2, ensure_ascii=False)
                print(f"  Successfully wrote raw task content to: {target_file_path}")
            except json.JSONDecodeError as e:
                print(f"  Error decoding JSON from {tasks_json_original_path} for raw copy: {e}")
            except Exception as e:
                print(f"  Error processing raw {tasks_json_original_path} for copy: {e}")
    # else: Main tasks.json not found, skipping its processing

    # Define source subdirectories to process (for raw_response extraction)
    source_subdirs_patterns = [
        os.path.join(task_folder, "Scripted_subtask_*"),
        os.path.join(task_folder, "Subtask_branches_*")
    ]

    files_processed_count = 0
    files_written_count = 0

    for pattern in source_subdirs_patterns:
        source_subdirs = glob.glob(pattern)
        for source_subdir in source_subdirs:
            if not os.path.isdir(source_subdir):
                continue
            
            source_subdir_basename = os.path.basename(source_subdir)
            target_subdir = os.path.join(target_task_dir, source_subdir_basename)
            
            print(f"  Processing subdirectory: {source_subdir_basename}")
            
            source_files = glob.glob(os.path.join(source_subdir, "*.json"))
            
            if not source_files:
                 print(f"    No .json files found in {source_subdir_basename}")
                 continue

            for source_file_path in source_files:
                files_processed_count += 1
                source_file_basename = os.path.basename(source_file_path)
                target_file_path = os.path.join(target_subdir, source_file_basename)
                
                try:
                    # print(f"    Reading: {source_file_basename}")
                    with open(source_file_path, 'r', encoding='utf-8') as f_in:
                        log_data = json.load(f_in)
                    
                    raw_response = log_data.get("raw_response")
                    
                    if raw_response is None:
                        print(f"    Warning: No 'raw_response' field found in {source_file_basename}, skipping.")
                        continue

                    parsed_data = clean_and_parse_raw_response(raw_response)

                    if parsed_data is not None:
                        # Ensure target directory exists
                        os.makedirs(target_subdir, exist_ok=True)
                        
                        # Write the parsed JSON data
                        with open(target_file_path, 'w', encoding='utf-8') as f_out:
                            json.dump(parsed_data, f_out, indent=2, ensure_ascii=False)
                        # print(f"      Successfully wrote: {target_file_path}")
                        files_written_count += 1
                    # else: # Already printed warning in clean_and_parse
                    #     print(f"      Failed to parse/clean raw_response from {source_file_basename}")

                except json.JSONDecodeError:
                    print(f"    Error: Could not decode JSON from source file {source_file_basename}. Skipping.")
                except Exception as e:
                    print(f"    Error processing file {source_file_basename}: {e}")

    print(f"Finished processing {task_folder_basename}. Processed {files_processed_count} files, wrote {files_written_count} files to {target_task_dir}.")


# Reuse find_task_folders logic (or similar) if needed for CLI
def find_task_folders(base_dir: str) -> List[str]:
    """Find all potential task folders in the base directory."""
    if not os.path.exists(base_dir) or not os.path.isdir(base_dir):
        return []
    potential_task_folders = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path):
            # Check criteria: presence of specific subfolders
            if (glob.glob(os.path.join(item_path, "Scripted_subtask_*")) or
                glob.glob(os.path.join(item_path, "Subtask_branches_*"))):
                potential_task_folders.append(item_path)
    return potential_task_folders

# --- Main Execution Block ---
def main():
    parser = argparse.ArgumentParser(
        description="Extract clean JSON responses from task log files into a parallel 'Middle_data' structure."
    )
    parser.add_argument(
        'task', 
        help="Source task folder path (e.g., data/TaskName_Timestamp) or just the TaskName_Timestamp part."
    )
    parser.add_argument(
        '--data-dir', 
        '-d', 
        default="Generate_branches/data", 
        help="Base directory containing the source task folders (default: Generate_branches/data)."
    )
    parser.add_argument(
        '--middle-dir', 
        '-m', 
        default=DEFAULT_MIDDLE_DATA_DIR,
        help=f"Base directory to write the output structures (default: {DEFAULT_MIDDLE_DATA_DIR})."
    )
    parser.add_argument(
        '--tasks-json-mode',
        default="extracted",
        choices=['extracted', 'raw'],
        help="Mode for processing tasks.json: 'extracted' for specific details (outputs tasks_details_extracted.json), "
             "'raw' to copy original content (outputs tasks_content_raw.json). Default is 'extracted'."
    )

    args = parser.parse_args()

    # Determine the full source task folder path
    source_task_folder = args.task
    if not os.path.isdir(source_task_folder):
        # If not a direct path, assume it's a name within data_dir
        source_task_folder = os.path.join(args.data_dir, args.task)
        if not os.path.isdir(source_task_folder):
             # Try finding it more dynamically if the exact name wasn't given
             possible_folders = glob.glob(os.path.join(args.data_dir, f"{args.task}*"))
             task_folders_found = [f for f in possible_folders if os.path.isdir(f)]
             if not task_folders_found:
                 print(f"Error: Cannot find source task folder '{args.task}' directly or within '{args.data_dir}'.")
                 return 1 # Indicate error
             elif len(task_folders_found) > 1:
                 print(f"Warning: Multiple folders match '{args.task}' in '{args.data_dir}'. Using the first: {os.path.basename(task_folders_found[0])}")
                 source_task_folder = task_folders_found[0]
             else:
                  source_task_folder = task_folders_found[0]


    # Ensure the base middle_data directory exists
    middle_data_base_path = args.middle_dir
    try:
        os.makedirs(middle_data_base_path, exist_ok=True)
        print(f"Ensured base output directory exists: {middle_data_base_path}")
    except OSError as e:
        print(f"Error: Could not create base output directory '{middle_data_base_path}': {e}")
        return 1 # Indicate error

    # Process the specified task folder
    process_task_folder_to_middle(source_task_folder, middle_data_base_path, args.tasks_json_mode)
    
    return 0 # Indicate success

if __name__ == "__main__":
    import sys
    sys.exit(main()) 
"""
Processes task folders generated by the FSM-LLM-Narrative system,
extracts the clean JSON content from the 'raw_response' field of log files,
and saves it to a parallel structure under a 'Middle_data' directory.
"""

import os
import json
import glob
import argparse
import re
from typing import Dict, List, Optional, Any

# Default base directory for the processed data
DEFAULT_MIDDLE_DATA_DIR = "Middle_data"

# Keys to keep in subtask objects when --filter-subtask-content is used
SUBTASK_KEYS_TO_KEEP_WHEN_FILTERING = {
    "name",
    "location",
    "trigger_conditions",
    "environment", 
    "interactive_environment_objects", 
    "interactive_npc", 
    "key_questions", # These are key_questions specific to the subtask, if generated by LLM
    "scene_end_state_reference"
}

def clean_and_parse_raw_response(raw_response_str: str) -> Optional[Any]:
    """Cleans markdown fences and parses JSON string."""
    if not isinstance(raw_response_str, str):
        print(f"Warning: raw_response is not a string ({type(raw_response_str)}), skipping parsing.")
        return None
        
    cleaned_response = raw_response_str.strip()
    if cleaned_response.startswith("```json"):
        cleaned_response = cleaned_response[7:]
    if cleaned_response.endswith("```"):
        cleaned_response = cleaned_response[:-3]
    cleaned_response = cleaned_response.strip()

    if not cleaned_response:
        # print("Warning: Cleaned response is empty, skipping parsing.")
        return None # Return None for empty strings after cleaning

    try:
        return json.loads(cleaned_response)
    except json.JSONDecodeError as e:
        print(f"Warning: JSONDecodeError - {e}. Content: {cleaned_response[:150]}...")
        return None
    except Exception as e:
        print(f"Warning: Unexpected error parsing JSON - {e}. Content: {cleaned_response[:150]}...")
        return None

def process_task_folder_to_middle(task_folder: str, middle_data_base: str, output_mode: str = "full", filter_subtask_content: bool = False):
    """
    Processes a single task folder.
    1. Extracts original task_info.
    2. Extracts and cleans raw_response JSON for all scripted and alternative subtasks.
    3. Saves a main_task_structured.json file that includes:
       - The original task_info as the base structure.
       - A 'generated_narrative_tree' key containing collected scripted and alternative subtasks.
    4. Optionally, can still save individual cleaned raw_responses if needed for debugging (current behavior).
    """
    if not os.path.isdir(task_folder):
        print(f"Error: Source task folder not found: {task_folder}")
        return

    task_folder_basename = os.path.basename(task_folder)
    target_task_dir = os.path.join(middle_data_base, task_folder_basename)
    os.makedirs(target_task_dir, exist_ok=True) # Ensure target task directory exists early

    print(f"Processing task folder: {task_folder_basename}")
    print(f"Target directory for structured output: {target_task_dir}")

    collected_scripted_subtasks: List[Dict[str, Any]] = []
    collected_alternative_subtasks: List[Dict[str, Any]] = []
    master_task_info: Optional[Dict[str, Any]] = None

    source_subdirs_map = {
        "Scripted_subtask_*": collected_scripted_subtasks,
        "Subtask_branches_*": collected_alternative_subtasks
    }

    files_processed_count = 0
    individual_files_written_count = 0

    for pattern, collection_list in source_subdirs_map.items():
        source_subdirs = glob.glob(os.path.join(task_folder, pattern))
        for source_subdir in source_subdirs:
            if not os.path.isdir(source_subdir):
                continue
            
            source_subdir_basename = os.path.basename(source_subdir)
            # Target for individual raw_response files (optional, can be kept for debugging)
            individual_target_subdir = os.path.join(target_task_dir, source_subdir_basename)
            
            print(f"  Processing subdirectory: {source_subdir_basename}")
            
            source_files = glob.glob(os.path.join(source_subdir, "*.json"))
            
            if not source_files:
                 print(f"    No .json files found in {source_subdir_basename}")
                 continue

            # Sort files to process them in a predictable order (e.g., by layer if present in name)
            source_files.sort() 

            for source_file_path in source_files:
                files_processed_count += 1
                source_file_basename = os.path.basename(source_file_path)
                
                try:
                    with open(source_file_path, 'r', encoding='utf-8') as f_in:
                        log_data = json.load(f_in)
                    
                    # Capture master_task_info from the first log file that has it
                    if master_task_info is None and "task_info" in log_data:
                        if isinstance(log_data["task_info"], dict):
                            master_task_info = log_data["task_info"]
                            print(f"    Captured master_task_info from: {source_file_basename}")
                        else:
                            print(f"    Warning: 'task_info' in {source_file_basename} is not a dictionary. Will try next file.")

                    raw_response = log_data.get("raw_response")
                    if raw_response is None:
                        # print(f"    Warning: No 'raw_response' field found in {source_file_basename}, skipping actual content extraction for this file.")
                        continue

                    parsed_data = clean_and_parse_raw_response(raw_response)

                    if parsed_data is not None:
                        # Apply content filtering if the flag is set
                        if filter_subtask_content and isinstance(parsed_data, dict):
                            parsed_data = {k: v for k, v in parsed_data.items() if k in SUBTASK_KEYS_TO_KEEP_WHEN_FILTERING}
                        
                        collection_list.append(parsed_data)
                        
                        if output_mode == "full": # Only write individual files if mode is 'full'
                            # Optional: Save individual cleaned raw_responses (current behavior)
                            # This can be useful for debugging or if direct access to individual LLM outputs is needed.
                            # If only the combined main_task_structured.json is desired, this block can be removed.
                            os.makedirs(individual_target_subdir, exist_ok=True)
                            individual_target_file_path = os.path.join(individual_target_subdir, source_file_basename)
                            with open(individual_target_file_path, 'w', encoding='utf-8') as f_out:
                                json.dump(parsed_data, f_out, indent=2, ensure_ascii=False)
                            individual_files_written_count += 1
                            # print(f"      Successfully wrote individual cleaned response: {individual_target_file_path}")
                    
                except json.JSONDecodeError:
                    print(f"    Error: Could not decode JSON from source log file {source_file_basename}. Skipping.")
                except Exception as e:
                    print(f"    Error processing file {source_file_basename}: {e}")

    if master_task_info:
        # Construct the main structured JSON
        main_structured_data = {}
        # Copy all keys from master_task_info to ensure structure matches Scripted_tasks.json
        for key, value in master_task_info.items():
            main_structured_data[key] = value
        
        # Add the generated narrative tree
        main_structured_data["generated_narrative_tree"] = {
            "scripted_subtasks": collected_scripted_subtasks,
            "alternative_subtasks": collected_alternative_subtasks # Renamed for clarity
        }
        
        # Save the main_task_structured.json
        structured_output_path = os.path.join(target_task_dir, "main_task_structured.json")
        try:
            with open(structured_output_path, 'w', encoding='utf-8') as f_out:
                json.dump(main_structured_data, f_out, indent=2, ensure_ascii=False)
            print(f"  Successfully wrote main structured output to: {structured_output_path}")
        except Exception as e:
            print(f"  Error writing main structured output {structured_output_path}: {e}")
    else:
        print(f"Warning: master_task_info could not be found for {task_folder_basename}. Cannot create main_task_structured.json.")

    print(f"Finished processing {task_folder_basename}. Processed {files_processed_count} log files.")
    if individual_files_written_count > 0 and output_mode == "full":
        print(f"Wrote {individual_files_written_count} individual cleaned LLM responses.")


# Reuse find_task_folders logic (or similar) if needed for CLI
def find_task_folders(base_dir: str) -> List[str]:
    """Find all potential task folders in the base directory."""
    if not os.path.exists(base_dir) or not os.path.isdir(base_dir):
        return []
    potential_task_folders = []
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path):
            # Check criteria: presence of specific subfolders
            if (glob.glob(os.path.join(item_path, "Scripted_subtask_*")) or
                glob.glob(os.path.join(item_path, "Subtask_branches_*"))):
                potential_task_folders.append(item_path)
    return potential_task_folders

# --- Main Execution Block ---
def main():
    """Main entry point for the command-line utility."""
    parser = argparse.ArgumentParser(
        description='Processes FSM-LLM narrative task folders and saves structured JSON to Middle_data directory.'
    )
    parser.add_argument(
        '--source-dir', '-s', default="Generate_branches/data",
        help='Base directory containing the task folders (e.g., data/TaskName_Timestamp).'
             ' Default: Generate_branches/data'
    )
    parser.add_argument(
        '--middle-dir', '-m', default=DEFAULT_MIDDLE_DATA_DIR,
        help=f'Base directory to save the processed data. Default: {DEFAULT_MIDDLE_DATA_DIR}'
    )
    parser.add_argument(
        '--task-folder', '-t',
        help='Specific task folder name (e.g., Beginning_20231026_100000) to process. '
             'If omitted, all valid task folders in --source-dir will be processed.'
    )
    parser.add_argument(
        '--output-mode',
        default="full",
        choices=['full', 'structured_only'],
        help='Output mode: "full" (default) saves both main structured file and individual LLM responses. "structured_only" saves only the main_task_structured.json.'
    )
    parser.add_argument(
        '--filter-subtask-content',
        action='store_true', # Makes it a boolean flag, default False
        help='If set, filters the content of each LLM-generated subtask to only include keys found in Scripted_tasks.json top-level entries (e.g., environment, interactive_npc).'
    )
    
    args = parser.parse_args()

    # Ensure the middle_data_base directory exists
    os.makedirs(args.middle_dir, exist_ok=True)

    if args.task_folder:
        # Process a single specified task folder
        task_folder_path = os.path.join(args.source_dir, args.task_folder)
        if os.path.isdir(task_folder_path) and is_valid_task_folder(task_folder_path): # Added validation
            process_task_folder_to_middle(task_folder_path, args.middle_dir, args.output_mode, args.filter_subtask_content)
        else:
            print(f"Error: Specified task folder {task_folder_path} is not a valid task folder or does not exist.")
    else:
        # Process all task folders in the source directory
        task_folders = find_task_folders(args.source_dir)
        if not task_folders:
            print(f"No task folders found in {args.source_dir}")
            return
        
        print(f"Found {len(task_folders)} task folder(s) to process in {args.source_dir}.")
        for task_folder_path in task_folders:
            process_task_folder_to_middle(task_folder_path, args.middle_dir, args.output_mode, args.filter_subtask_content)

    print("\nProcessing complete.")

def is_valid_task_folder(folder_path: str) -> bool:
    """Checks if a folder is a valid task folder based on subdirectories."""
    if not os.path.isdir(folder_path):
        return False
    if (glob.glob(os.path.join(folder_path, "Scripted_subtask_*")) or
        glob.glob(os.path.join(folder_path, "Subtask_branches_*"))):
        return True
    return False

if __name__ == "__main__":
    import sys
    sys.exit(main()) 